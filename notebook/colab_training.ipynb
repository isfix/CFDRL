{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "title",
            "metadata": {},
            "source": [
                "# Phase 27: Research-Driven Trading System â€” Colab Training\n",
                "\n",
                "**Fixes Applied:**\n",
                "- ðŸ”´ P0: True DDQN (target net, soft-update Ï„=0.005)\n",
                "- ðŸ”´ P0: Corrected volatility scaling (1/vol inverse)\n",
                "- ðŸŸ  P1: Sequence 240, No warm-start, Trade-only Sharpe\n",
                "- ðŸŸ¡ P2: SAE encoder (21â†’12 features), LR scheduler, Îµ=0.98\n",
                "\n",
                "**Data:** `/content/drive/MyDrive/data/{symbol}.csv`\n",
                "\n",
                "### GPU: Runtime > Change runtime type > **T4 GPU**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "!pip install pandas_ta tqdm matplotlib -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch, torch.nn as nn, torch.optim as optim\n",
                "import pandas as pd, pandas_ta as ta, numpy as np\n",
                "import os, random, matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class Settings:\n",
                "    PAIR_CONFIGS = {\n",
                "        'XAUUSD': {'spread': 0.20, 'commission': 0.0, 'scaling_factor': 5.0, 'contract_size': 100},\n",
                "        'EURUSD': {'spread': 0.0001, 'commission': 0.0, 'scaling_factor': 10000.0, 'contract_size': 100000},\n",
                "        'GBPUSD': {'spread': 0.0002, 'commission': 0.0, 'scaling_factor': 10000.0, 'contract_size': 100000},\n",
                "    }\n",
                "    PAIRS = list(PAIR_CONFIGS.keys())\n",
                "    SEQUENCE_LENGTH = 240  # P1: Fischer & Krauss\n",
                "    FEATURES = [\n",
                "        'ret_close_1', 'ret_close_2', 'ret_close_5', 'ret_close_10', 'ret_close_20',\n",
                "        'body_ratio', 'upper_wick_ratio', 'lower_wick_ratio', 'range_ratio',\n",
                "        'volatility_ratio', 'atr_normalized',\n",
                "        'rsi_14', 'macd_signal_dist', 'roc_10',\n",
                "        'dist_ema_50', 'dist_ema_200', 'ema_slope_50',\n",
                "        'bb_position', 'bb_width',\n",
                "        'adx_normalized', 'di_diff',\n",
                "    ]\n",
                "    INPUT_DIM = len(FEATURES)  # 21\n",
                "    SAE_DIM = 12              # P2: SAE compressed\n",
                "    HIDDEN_DIM = 64\n",
                "    NUM_LAYERS = 2\n",
                "    DROPOUT = 0.1\n",
                "    OUTPUT_DIM = 7\n",
                "    ACTION_MAP = [-1.0, -0.66, -0.33, 0.0, 0.33, 0.66, 1.0]\n",
                "    MAX_LOT_SIZE = 0.05\n",
                "    EPOCHS = 20\n",
                "    BATCH_SIZE = 64\n",
                "    LEARNING_RATE = 0.0001\n",
                "    GAMMA = 0.99\n",
                "    EPSILON_START = 1.0\n",
                "    EPSILON_DECAY = 0.98     # P2: Slower\n",
                "    EPSILON_MIN = 0.01\n",
                "    TARGET_UPDATE_FREQ = 100  # P0: DDQN\n",
                "    TAU = 0.005\n",
                "    LR_SCHEDULER_PATIENCE = 3\n",
                "    LR_SCHEDULER_FACTOR = 0.5\n",
                "    PER_ALPHA = 0.6\n",
                "    PER_BETA_START = 0.4\n",
                "    SIGMA_TARGET = 0.02\n",
                "    VOLATILITY_WINDOW = 20\n",
                "    TRANSACTION_COST_BPS = 10\n",
                "    TRAIN_WINDOW = 60000\n",
                "    VAL_WINDOW = 10000\n",
                "    TEST_WINDOW = 10000\n",
                "    EARLY_STOP_PATIENCE = 10\n",
                "    STANDARDIZE_WINDOW = 60\n",
                "\n",
                "print(f'Config: {Settings.INPUT_DIM} feats â†’ SAE({Settings.SAE_DIM}) â†’ LSTM({Settings.HIDDEN_DIM}) â†’ {Settings.OUTPUT_DIM} actions')\n",
                "print(f'Device: {torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "brain",
            "metadata": {},
            "outputs": [],
            "source": [
                "class QNetwork(nn.Module):\n",
                "    def __init__(self, input_dim=Settings.INPUT_DIM, sae_dim=Settings.SAE_DIM,\n",
                "                 hidden_dim=Settings.HIDDEN_DIM, num_layers=Settings.NUM_LAYERS,\n",
                "                 dropout=Settings.DROPOUT, output_dim=Settings.OUTPUT_DIM):\n",
                "        super(QNetwork, self).__init__()\n",
                "        self.sae_encoder = nn.Sequential(\n",
                "            nn.Linear(input_dim, input_dim), nn.ReLU(),\n",
                "            nn.Linear(input_dim, sae_dim), nn.ReLU())\n",
                "        self.lstm = nn.LSTM(input_size=sae_dim, hidden_size=hidden_dim,\n",
                "                           num_layers=num_layers, dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
                "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(hidden_dim, hidden_dim // 2), nn.ReLU(), nn.Dropout(dropout),\n",
                "            nn.Linear(hidden_dim // 2, output_dim))\n",
                "\n",
                "    def forward(self, x):\n",
                "        bs, sl, _ = x.size()\n",
                "        encoded = self.sae_encoder(x.reshape(bs * sl, -1)).reshape(bs, sl, -1)\n",
                "        lstm_out, _ = self.lstm(encoded)\n",
                "        return self.fc(self.layer_norm(lstm_out[:, -1, :]))\n",
                "\n",
                "print('QNetwork with SAE encoder loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "per",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SumTree:\n",
                "    def __init__(self, capacity):\n",
                "        self.capacity = capacity\n",
                "        self.tree = np.zeros(2 * capacity - 1)\n",
                "        self.data = np.zeros(capacity, dtype=object)\n",
                "        self.write = 0\n",
                "        self.count = 0\n",
                "    def _propagate(self, idx, change):\n",
                "        parent = (idx - 1) // 2\n",
                "        self.tree[parent] += change\n",
                "        if parent != 0: self._propagate(parent, change)\n",
                "    def _retrieve(self, idx, s):\n",
                "        left = 2 * idx + 1\n",
                "        if left >= len(self.tree): return idx\n",
                "        if s <= self.tree[left]: return self._retrieve(left, s)\n",
                "        return self._retrieve(left + 1, s - self.tree[left])\n",
                "    def total(self): return self.tree[0]\n",
                "    def add(self, p, data):\n",
                "        idx = self.write + self.capacity - 1\n",
                "        self.data[self.write] = data\n",
                "        self.update(idx, p)\n",
                "        self.write = (self.write + 1) % self.capacity\n",
                "        if self.count < self.capacity: self.count += 1\n",
                "    def update(self, idx, p):\n",
                "        change = p - self.tree[idx]\n",
                "        self.tree[idx] = p\n",
                "        self._propagate(idx, change)\n",
                "    def get(self, s):\n",
                "        idx = self._retrieve(0, s)\n",
                "        return (idx, self.tree[idx], self.data[idx - self.capacity + 1])\n",
                "\n",
                "class PrioritizedReplayBuffer:\n",
                "    def __init__(self, capacity=10000, alpha=0.6):\n",
                "        self.tree = SumTree(capacity)\n",
                "        self.alpha = alpha\n",
                "        self.capacity = capacity\n",
                "    def push(self, *args):\n",
                "        max_p = max(np.max(self.tree.tree[-self.tree.capacity:]), 1.0)\n",
                "        self.tree.add(max_p, args)\n",
                "    def sample(self, batch_size, beta=0.4):\n",
                "        batch, idxs, priorities = [], [], []\n",
                "        seg = self.tree.total() / batch_size\n",
                "        for i in range(batch_size):\n",
                "            s = random.uniform(seg * i, seg * (i + 1))\n",
                "            idx, p, data = self.tree.get(s)\n",
                "            if data is None or (isinstance(data, (int, float)) and data == 0):\n",
                "                vi = random.randint(0, self.tree.count - 1)\n",
                "                data = self.tree.data[vi]; idx = vi + self.capacity - 1; p = self.tree.tree[idx]\n",
                "            priorities.append(p); batch.append(data); idxs.append(idx)\n",
                "        sp = np.array(priorities) / self.tree.total()\n",
                "        isw = np.power(self.tree.count * sp, -beta); isw /= isw.max()\n",
                "        s, ns, cp, np_, v = zip(*batch)\n",
                "        return np.array(s), np.array(ns), np.array(cp, dtype=np.float32), np.array(np_, dtype=np.float32), np.array(v, dtype=np.float32), idxs, np.array(isw, dtype=np.float32)\n",
                "    def update_priorities(self, idxs, errors):\n",
                "        for idx, e in zip(idxs, errors): self.tree.update(idx, (abs(e) + 1e-5) ** self.alpha)\n",
                "    def __len__(self): return self.tree.count\n",
                "\n",
                "print('PER loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "features",
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_features(df):\n",
                "    if df.empty: return df\n",
                "    df = df.copy()\n",
                "    df['mid_price'] = (df['high'] + df['low']) / 2.0\n",
                "    df['atr'] = ta.atr(df['high'], df['low'], df['close'], length=14)\n",
                "    df['atr'] = df['atr'].fillna(method='bfill').fillna(df['close'] * 0.001)\n",
                "    df['volatility'] = df['atr'] / df['close']\n",
                "    for lag in [1, 2, 5, 10, 20]:\n",
                "        raw_ret = df['close'].pct_change(lag)\n",
                "        rm = raw_ret.rolling(Settings.STANDARDIZE_WINDOW).mean()\n",
                "        rs = raw_ret.rolling(Settings.STANDARDIZE_WINDOW).std().replace(0, np.nan).fillna(raw_ret.std())\n",
                "        df[f'ret_close_{lag}'] = (raw_ret - rm) / rs\n",
                "    df['body_ratio'] = (df['close'] - df['open']) / df['atr']\n",
                "    df['upper_wick_ratio'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['atr']\n",
                "    df['lower_wick_ratio'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['atr']\n",
                "    df['range_ratio'] = (df['high'] - df['low']) / df['atr']\n",
                "    rv = df['close'].pct_change().rolling(Settings.VOLATILITY_WINDOW).std()\n",
                "    rvm = rv.rolling(100).mean().replace(0, np.nan).fillna(rv.mean())\n",
                "    df['volatility_ratio'] = rv / rvm\n",
                "    df['atr_normalized'] = df['atr'] / df['close']\n",
                "    rsi = ta.rsi(df['close'], length=14)\n",
                "    df['rsi_14'] = rsi / 100.0 if rsi is not None else 0.0\n",
                "    macd_df = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
                "    if macd_df is not None and not macd_df.empty:\n",
                "        mc = [c for c in macd_df.columns if 'MACDh' in c or 'MACD_' in c]\n",
                "        sc = [c for c in macd_df.columns if 'MACDs' in c]\n",
                "        df['macd_signal_dist'] = (macd_df[mc[0]] - macd_df[sc[0]]) / df['atr'] if mc and sc else 0.0\n",
                "    else: df['macd_signal_dist'] = 0.0\n",
                "    roc = ta.roc(df['close'], length=10)\n",
                "    df['roc_10'] = roc / 100.0 if roc is not None else 0.0\n",
                "    e50 = ta.ema(df['close'], length=50)\n",
                "    e200 = ta.ema(df['close'], length=200)\n",
                "    df['ema_200'] = e200\n",
                "    df['dist_ema_50'] = (df['close'] - e50) / df['atr']\n",
                "    df['dist_ema_200'] = (df['close'] - e200) / df['atr']\n",
                "    df['ema_slope_50'] = (e50 - e50.shift(5)) / df['atr']\n",
                "    bb = ta.bbands(df['close'], length=20, std=2)\n",
                "    if bb is not None and not bb.empty:\n",
                "        bbl, bbm, bbu = bb.iloc[:, 0], bb.iloc[:, 1], bb.iloc[:, 2]\n",
                "        bbr = (bbu - bbl).replace(0, np.nan).fillna(1e-8)\n",
                "        df['bb_position'] = (df['close'] - bbl) / bbr\n",
                "        df['bb_width'] = (bbu - bbl) / df['close']\n",
                "    else: df['bb_position'] = 0.5; df['bb_width'] = 0.0\n",
                "    adx_df = ta.adx(df['high'], df['low'], df['close'], length=14)\n",
                "    if adx_df is not None and not adx_df.empty:\n",
                "        df['adx_normalized'] = adx_df.iloc[:, 0] / 100.0\n",
                "        df['di_diff'] = (adx_df.iloc[:, 1] - adx_df.iloc[:, 2]) / 100.0 if len(adx_df.columns) >= 3 else 0.0\n",
                "    else: df['adx_normalized'] = 0.0; df['di_diff'] = 0.0\n",
                "    df.fillna(0, inplace=True)\n",
                "    df.replace([np.inf, -np.inf], 0, inplace=True)\n",
                "    for f in Settings.FEATURES:\n",
                "        if f in df.columns: df[f] = df[f].clip(-10, 10)\n",
                "    df = df.iloc[250:]\n",
                "    return df\n",
                "\n",
                "print('Feature engineering loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dataset",
            "metadata": {},
            "outputs": [],
            "source": [
                "class TradingDataset(Dataset):\n",
                "    def __init__(self, feature_data, close_prices, volatilities, seq_len):\n",
                "        self.feature_data = feature_data\n",
                "        self.close_prices = close_prices\n",
                "        self.volatilities = volatilities\n",
                "        self.seq_len = seq_len\n",
                "        self.valid_indices = range(seq_len, len(feature_data) - 1)\n",
                "    def __len__(self): return len(self.valid_indices)\n",
                "    def __getitem__(self, idx):\n",
                "        i = self.valid_indices[idx]\n",
                "        return {\n",
                "            'state': torch.FloatTensor(self.feature_data[i - self.seq_len : i].copy()),\n",
                "            'next_state': torch.FloatTensor(self.feature_data[i - self.seq_len + 1 : i + 1].copy()),\n",
                "            'curr_price': torch.tensor(self.close_prices[i-1], dtype=torch.float32),\n",
                "            'next_price': torch.tensor(self.close_prices[i], dtype=torch.float32),\n",
                "            'volatility': torch.tensor(self.volatilities[i-1], dtype=torch.float32)\n",
                "        }\n",
                "print('Dataset loaded.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_fn",
            "metadata": {},
            "outputs": [],
            "source": [
                "def soft_update(policy_net, target_net, tau=Settings.TAU):\n",
                "    for tp, pp in zip(target_net.parameters(), policy_net.parameters()):\n",
                "        tp.data.copy_(tau * pp.data + (1.0 - tau) * tp.data)\n",
                "\n",
                "def train_model(symbol, csv_path):\n",
                "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "    print(f'\\nTraining {symbol} on {device}...')\n",
                "    df = pd.read_csv(csv_path, parse_dates=['time'], index_col='time')\n",
                "    df = prepare_features(df)\n",
                "    if df.empty: print('No data.'); return\n",
                "    print(f'Data: {len(df)} bars')\n",
                "\n",
                "    total = len(df)\n",
                "    wt = Settings.TRAIN_WINDOW + Settings.VAL_WINDOW + Settings.TEST_WINDOW\n",
                "    if total < wt:\n",
                "        te = int(total * 0.7); ve = int(total * 0.85)\n",
                "        windows = [(0, te, ve, total)]\n",
                "    else:\n",
                "        windows = []; s = 0\n",
                "        while s + wt <= total:\n",
                "            t_ = s + Settings.TRAIN_WINDOW; v_ = t_ + Settings.VAL_WINDOW; ts_ = v_ + Settings.TEST_WINDOW\n",
                "            windows.append((s, t_, v_, ts_)); s += Settings.TEST_WINDOW\n",
                "        if len(windows) > 5: windows = windows[-5:]\n",
                "    print(f'Windows: {len(windows)}')\n",
                "\n",
                "    best_val_sharpe = -np.inf\n",
                "    best_model_state = None\n",
                "    loss_fn = nn.MSELoss(reduction='none')\n",
                "    SF = Settings.PAIR_CONFIGS[symbol]['scaling_factor']\n",
                "\n",
                "    for w_idx, (ws, wte, wve, wtse) in enumerate(windows):\n",
                "        print(f'\\n--- Window {w_idx+1}/{len(windows)} ---')\n",
                "        df_t = df.iloc[ws:wte]; df_v = df.iloc[wte:wve]\n",
                "        tf = df_t[Settings.FEATURES].values; tp = df_t['mid_price'].values\n",
                "        tv = df_t['volatility_ratio'].values if 'volatility_ratio' in df_t.columns else np.ones(len(df_t))\n",
                "        tds = TradingDataset(tf, tp, tv, Settings.SEQUENCE_LENGTH)\n",
                "        vf = df_v[Settings.FEATURES].values; vp = df_v['mid_price'].values\n",
                "        vv = df_v['volatility_ratio'].values if 'volatility_ratio' in df_v.columns else np.ones(len(df_v))\n",
                "        vds = TradingDataset(vf, vp, vv, Settings.SEQUENCE_LENGTH)\n",
                "        if len(tds) < Settings.BATCH_SIZE: continue\n",
                "\n",
                "        mem = PrioritizedReplayBuffer(capacity=len(tds), alpha=Settings.PER_ALPHA)\n",
                "        tl = DataLoader(tds, batch_size=4096, shuffle=False)\n",
                "        for b in tqdm(tl, desc='Fill'):\n",
                "            s_, ns_, cp_, np__, v_ = b['state'].numpy(), b['next_state'].numpy(), b['curr_price'].numpy(), b['next_price'].numpy(), b['volatility'].numpy()\n",
                "            for i in range(len(s_)): mem.push(s_[i], ns_[i], cp_[i], np__[i], v_[i])\n",
                "\n",
                "        # P1: No warm-start â€” fresh network per window\n",
                "        policy_net = QNetwork().to(device)\n",
                "        # P0: Target network\n",
                "        target_net = QNetwork().to(device)\n",
                "        target_net.load_state_dict(policy_net.state_dict()); target_net.eval()\n",
                "\n",
                "        optimizer = optim.Adam(policy_net.parameters(), lr=Settings.LEARNING_RATE)\n",
                "        # P2: LR scheduler\n",
                "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
                "            patience=Settings.LR_SCHEDULER_PATIENCE, factor=Settings.LR_SCHEDULER_FACTOR)\n",
                "\n",
                "        epsilon = Settings.EPSILON_START\n",
                "        steps = len(tds) // Settings.BATCH_SIZE\n",
                "        best_vl = np.inf; pat = 0; gs = 0\n",
                "        am = torch.tensor(Settings.ACTION_MAP, device=device)\n",
                "\n",
                "        for epoch in range(Settings.EPOCHS):\n",
                "            tl_ = 0\n",
                "            beta = Settings.PER_BETA_START + (1.0 - Settings.PER_BETA_START) * (epoch / Settings.EPOCHS)\n",
                "            pbar = tqdm(range(steps), desc=f'W{w_idx+1} E{epoch+1}')\n",
                "            for _ in pbar:\n",
                "                ss, nss, cps, nps, vs, idxs, isw = mem.sample(Settings.BATCH_SIZE, beta)\n",
                "                st = torch.FloatTensor(ss).to(device)\n",
                "                nst = torch.FloatTensor(nss).to(device)\n",
                "                cpt = torch.FloatTensor(cps).to(device)\n",
                "                npt = torch.FloatTensor(nps).to(device)\n",
                "                vt = torch.FloatTensor(vs).to(device)\n",
                "                wt_ = torch.FloatTensor(isw).to(device)\n",
                "                bs = st.size(0)\n",
                "                if random.random() < epsilon:\n",
                "                    at = torch.randint(0, Settings.OUTPUT_DIM, (bs,), device=device)\n",
                "                else:\n",
                "                    with torch.no_grad(): at = torch.argmax(policy_net(st), dim=1)\n",
                "                pos = am[at]\n",
                "                pc = (npt - cpt) * SF\n",
                "                # P0: Corrected vol scaling\n",
                "                vs_ = vt.clamp(min=0.2)\n",
                "                vsc = (1.0 / vs_).clamp(0.2, 3.0)\n",
                "                cost = Settings.TRANSACTION_COST_BPS * 0.0001 * SF * torch.abs(pos)\n",
                "                reward = (vsc * pos * pc - cost).clamp(-5.0, 5.0)\n",
                "                cq = policy_net(st).gather(1, at.unsqueeze(1)).squeeze(1)\n",
                "                with torch.no_grad():\n",
                "                    # P0: True DDQN\n",
                "                    na = policy_net(nst).argmax(1)\n",
                "                    nq = target_net(nst).gather(1, na.unsqueeze(1)).squeeze(1)\n",
                "                    tq = reward + Settings.GAMMA * nq\n",
                "                loss = (loss_fn(cq, tq) * wt_).mean()\n",
                "                optimizer.zero_grad(); loss.backward()\n",
                "                torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)\n",
                "                optimizer.step()\n",
                "                mem.update_priorities(idxs, torch.abs(tq - cq).detach().cpu().numpy())\n",
                "                tl_ += loss.item(); gs += 1\n",
                "                if gs % Settings.TARGET_UPDATE_FREQ == 0:\n",
                "                    soft_update(policy_net, target_net)\n",
                "            if epsilon > Settings.EPSILON_MIN: epsilon *= Settings.EPSILON_DECAY\n",
                "            al = tl_ / steps\n",
                "            scheduler.step(al)\n",
                "            print(f'W{w_idx+1} E{epoch+1}: Loss={al:.6f} Eps={epsilon:.4f} LR={optimizer.param_groups[0][\"lr\"]:.2e}')\n",
                "\n",
                "            # Validation (P1: trade-only Sharpe)\n",
                "            policy_net.eval()\n",
                "            vl = DataLoader(vds, batch_size=Settings.BATCH_SIZE, shuffle=False)\n",
                "            all_r = []; trades = 0\n",
                "            with torch.no_grad():\n",
                "                for vb in vl:\n",
                "                    vs_t = vb['state'].to(device).float()\n",
                "                    cpv = vb['curr_price'].to(device).float()\n",
                "                    npv = vb['next_price'].to(device).float()\n",
                "                    a = torch.argmax(policy_net(vs_t), dim=1)\n",
                "                    p = am[a]; r = p * (npv - cpv) * SF\n",
                "                    trades += (a != 3).sum().item()\n",
                "                    all_r.extend(r.cpu().numpy().tolist())\n",
                "            ra = np.array(all_r)\n",
                "            tr = ra[ra != 0]  # P1: trade-only\n",
                "            sharpe = (np.mean(tr) / np.std(tr) * np.sqrt(252*12*24)) if len(tr) > 1 and np.std(tr) > 0 else 0\n",
                "            print(f'[VAL] Trades: {trades}, PnL: {ra.sum():.5f}, Sharpe: {sharpe:.4f}')\n",
                "            policy_net.train()\n",
                "            if al < best_vl:\n",
                "                best_vl = al; pat = 0\n",
                "                if sharpe > best_val_sharpe:\n",
                "                    best_val_sharpe = sharpe\n",
                "                    best_model_state = {k: v.clone() for k, v in policy_net.state_dict().items()}\n",
                "            else:\n",
                "                pat += 1\n",
                "                if pat >= Settings.EARLY_STOP_PATIENCE: print(f'Early stop E{epoch+1}'); break\n",
                "\n",
                "    sp = os.path.join(os.path.dirname(csv_path), f'{symbol}_brain.pth')\n",
                "    torch.save(best_model_state if best_model_state else policy_net.state_dict(), sp)\n",
                "    print(f'Model saved: {sp} (Sharpe: {best_val_sharpe:.4f})')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "execute",
            "metadata": {},
            "outputs": [],
            "source": [
                "# âš¡ SELECT PAIRS:\n",
                "pairs = ['EURUSD']\n",
                "drive_data = '/content/drive/MyDrive/data'\n",
                "\n",
                "for sym in pairs:\n",
                "    csv = os.path.join(drive_data, f'{sym}.csv')\n",
                "    if os.path.exists(csv):\n",
                "        print(f'Found: {csv}')\n",
                "        train_model(sym, csv)\n",
                "    else:\n",
                "        print(f'{sym} data not found.')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}